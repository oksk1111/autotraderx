"""
Í∞ïÌôîÌïôÏäµ Ìä∏Î†àÏù¥Îî© ÏóêÏù¥Ï†ÑÌä∏ (Layer 3)
PPO(Proximal Policy Optimization) ÏïåÍ≥†Î¶¨Ï¶ò ÏÇ¨Ïö©

ÏÉÅÌÉú Í≥µÍ∞Ñ:
- ÏãúÏû• features (46Ï∞®Ïõê)
- Layer 1 Ïã†Ìò∏ (3Ï∞®Ïõê - BUY/SELL/HOLD)
- Layer 2 Ïã†Ìò∏ (3Ï∞®Ïõê)
Ï¥ù 52Ï∞®Ïõê

ÌñâÎèô Í≥µÍ∞Ñ:
- 0: HOLD
- 1: BUY
- 2: SELL
"""

from __future__ import annotations

import os
import numpy as np
from pathlib import Path
from typing import Tuple, Optional, Dict

from app.core.logging import get_logger

logger = get_logger(__name__)


class RLTradingAgent:
    """
    Í∞ïÌôîÌïôÏäµ Í∏∞Î∞ò Ìä∏Î†àÏù¥Îî© ÏóêÏù¥Ï†ÑÌä∏
    
    PPO ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏµúÏ†ÅÏùò Îß§Îß§ ÌÉÄÏù¥Î∞ç ÌïôÏäµ
    """
    
    def __init__(self, model_path: Optional[str] = None):
        """
        Args:
            model_path: ÌïôÏäµÎêú Î™®Îç∏ Í≤ΩÎ°ú (ÏóÜÏúºÎ©¥ Ïã†Í∑ú ÏÉùÏÑ±)
        """
        self.model_path = model_path or self._get_default_model_path()
        self.model = None
        self.is_trained = False
        
        # ÏÉÅÌÉú/ÌñâÎèô Í≥µÍ∞Ñ ÏÑ§Ï†ï
        self.state_dim = 52  # 46 features + 3 tech + 3 trend
        self.action_dim = 3  # HOLD, BUY, SELL
        
        # ÌñâÎèô Îß§Ìïë
        self.action_to_signal = {
            0: "HOLD",
            1: "BUY",
            2: "SELL"
        }
        
        self._load_model()
    
    def _get_default_model_path(self) -> str:
        """Í∏∞Î≥∏ Î™®Îç∏ Ï†ÄÏû• Í≤ΩÎ°ú"""
        return "/app/models/rl_agent_ppo.zip"
    
    def _load_model(self):
        """ÌïôÏäµÎêú Î™®Îç∏ Î°úÎî©"""
        try:
            # stable-baselines3 ÏûÑÌè¨Ìä∏
            from stable_baselines3 import PPO
            
            if os.path.exists(self.model_path):
                self.model = PPO.load(self.model_path)
                self.is_trained = True
                logger.info(f"‚úÖ RL Agent loaded from {self.model_path}")
            else:
                logger.warning(f"‚ö†Ô∏è RL model not found at {self.model_path}")
                logger.info("RL AgentÎäî ÌïôÏäµ ÌõÑ ÏÇ¨Ïö© Í∞ÄÎä•Ìï©ÎãàÎã§.")
                self.is_trained = False
        
        except ImportError:
            logger.warning("stable-baselines3 not installed. RL Agent disabled.")
            self.is_trained = False
        except Exception as e:
            logger.error(f"Failed to load RL model: {e}")
            self.is_trained = False
    
    def predict(self, state: np.ndarray) -> Tuple[str, float]:
        """
        Ï£ºÏñ¥ÏßÑ ÏÉÅÌÉúÏóêÏÑú ÏµúÏ†Å ÌñâÎèô ÏòàÏ∏°
        
        Args:
            state: ÏÉÅÌÉú Î≤°ÌÑ∞ (52Ï∞®Ïõê)
        
        Returns:
            (action, confidence)
            - action: BUY, SELL, HOLD
            - confidence: 0.0 ~ 1.0
        """
        if not self.is_trained or self.model is None:
            # ÌïôÏäµ Ïïà Îêú Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í
            return "HOLD", 0.3
        
        try:
            # ÏÉÅÌÉú Ï∞®Ïõê Í≤ÄÏ¶ù
            if state.shape[0] != self.state_dim:
                logger.error(
                    f"Invalid state dimension: {state.shape[0]} "
                    f"(expected {self.state_dim})"
                )
                return "HOLD", 0.3
            
            # ÏòàÏ∏° ÏàòÌñâ
            action, _states = self.model.predict(state, deterministic=True)
            
            # Ïã†Î¢∞ÎèÑ Í≥ÑÏÇ∞ (Ï†ïÏ±Ö ÎÑ§Ìä∏ÏõåÌÅ¨Ïùò ÌôïÎ•† Î∂ÑÌè¨ ÏÇ¨Ïö©)
            # PPOÎäî stochastic policyÏù¥ÎØÄÎ°ú action probability Ï∂îÏ∂ú
            obs_tensor = self.model.policy.obs_to_tensor(state.reshape(1, -1))[0]
            distribution = self.model.policy.get_distribution(obs_tensor)
            probs = distribution.distribution.probs.detach().cpu().numpy()[0]
            
            confidence = float(probs[action])
            signal = self.action_to_signal[int(action)]
            
            logger.debug(f"[RL] Action={signal}, Confidence={confidence:.1%}")
            
            return signal, confidence
        
        except Exception as e:
            logger.error(f"RL prediction error: {e}")
            return "HOLD", 0.3
    
    def get_action_probs(self, state: np.ndarray) -> Dict[str, float]:
        """
        Î™®Îì† ÌñâÎèôÏóê ÎåÄÌïú ÌôïÎ•† Î∂ÑÌè¨ Î∞òÌôò
        
        Args:
            state: ÏÉÅÌÉú Î≤°ÌÑ∞
        
        Returns:
            {'HOLD': 0.6, 'BUY': 0.3, 'SELL': 0.1}
        """
        if not self.is_trained or self.model is None:
            return {"HOLD": 0.6, "BUY": 0.2, "SELL": 0.2}
        
        try:
            obs_tensor = self.model.policy.obs_to_tensor(state.reshape(1, -1))[0]
            distribution = self.model.policy.get_distribution(obs_tensor)
            probs = distribution.distribution.probs.detach().cpu().numpy()[0]
            
            return {
                "HOLD": float(probs[0]),
                "BUY": float(probs[1]),
                "SELL": float(probs[2])
            }
        
        except Exception as e:
            logger.error(f"Failed to get action probs: {e}")
            return {"HOLD": 0.6, "BUY": 0.2, "SELL": 0.2}
    
    def is_available(self) -> bool:
        """RL ÏóêÏù¥Ï†ÑÌä∏ ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä"""
        return self.is_trained and self.model is not None


# === ÌïôÏäµÏö© Ìó¨Ìçº Ìï®Ïàò ===

def train_rl_agent(
    env,
    total_timesteps: int = 100_000,
    save_path: Optional[str] = None,
    learning_rate: float = 3e-4,
    n_steps: int = 2048,
    batch_size: int = 64,
    n_epochs: int = 10,
    gamma: float = 0.99,
    verbose: int = 1
) -> RLTradingAgent:
    """
    RL ÏóêÏù¥Ï†ÑÌä∏ ÌïôÏäµ
    
    Args:
        env: Ìä∏Î†àÏù¥Îî© ÌôòÍ≤Ω (Gym interface)
        total_timesteps: Ï¥ù ÌïôÏäµ Ïä§ÌÖù Ïàò
        save_path: Î™®Îç∏ Ï†ÄÏû• Í≤ΩÎ°ú
        learning_rate: ÌïôÏäµÎ•†
        n_steps: Ìïú ÏóÖÎç∞Ïù¥Ìä∏Îãπ Ïä§ÌÖù Ïàò
        batch_size: Î∞∞Ïπò ÌÅ¨Í∏∞
        n_epochs: PPO ÏóÖÎç∞Ïù¥Ìä∏ epoch Ïàò
        gamma: Ìï†Ïù∏Ïú®
        verbose: Î°úÍ∑∏ Î†àÎ≤®
    
    Returns:
        ÌïôÏäµÎêú RLTradingAgent
    """
    try:
        from stable_baselines3 import PPO
        from stable_baselines3.common.callbacks import CheckpointCallback
        
        # Î™®Îç∏ ÏÉùÏÑ±
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=learning_rate,
            n_steps=n_steps,
            batch_size=batch_size,
            n_epochs=n_epochs,
            gamma=gamma,
            verbose=verbose,
            tensorboard_log="/app/logs/tensorboard/"
        )
        
        # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÏΩúÎ∞±
        save_path = save_path or "/app/models/rl_agent_ppo"
        checkpoint_callback = CheckpointCallback(
            save_freq=10000,
            save_path="/app/models/checkpoints/",
            name_prefix="rl_agent"
        )
        
        logger.info(f"üöÄ Starting RL training for {total_timesteps} timesteps...")
        
        # ÌïôÏäµ ÏãúÏûë
        model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            progress_bar=True
        )
        
        # Î™®Îç∏ Ï†ÄÏû•
        model.save(save_path)
        logger.info(f"‚úÖ Model saved to {save_path}.zip")
        
        # RLTradingAgentÎ°ú ÎûòÌïë
        agent = RLTradingAgent(model_path=f"{save_path}.zip")
        
        return agent
    
    except ImportError:
        logger.error(
            "stable-baselines3 not installed. "
            "Install with: pip install stable-baselines3"
        )
        raise
    except Exception as e:
        logger.error(f"Training failed: {e}")
        raise


def evaluate_rl_agent(
    agent: RLTradingAgent,
    env,
    n_episodes: int = 10
) -> Dict[str, float]:
    """
    RL ÏóêÏù¥Ï†ÑÌä∏ ÏÑ±Îä• ÌèâÍ∞Ä
    
    Args:
        agent: ÌïôÏäµÎêú ÏóêÏù¥Ï†ÑÌä∏
        env: ÌèâÍ∞Ä ÌôòÍ≤Ω
        n_episodes: ÌèâÍ∞Ä ÏóêÌîºÏÜåÎìú Ïàò
    
    Returns:
        ÌèâÍ∞Ä ÏßÄÌëú ÎîïÏÖîÎÑàÎ¶¨
    """
    total_rewards = []
    total_profits = []
    win_counts = 0
    
    for episode in range(n_episodes):
        obs = env.reset()
        done = False
        episode_reward = 0
        
        while not done:
            # ÏóêÏù¥Ï†ÑÌä∏ ÏòàÏ∏°
            action_signal, confidence = agent.predict(obs)
            
            # ÌñâÎèô Î≥ÄÌôò (signal ‚Üí action_id)
            action_map = {"HOLD": 0, "BUY": 1, "SELL": 2}
            action = action_map[action_signal]
            
            # ÌôòÍ≤Ω Ïä§ÌÖù
            obs, reward, done, info = env.step(action)
            episode_reward += reward
        
        total_rewards.append(episode_reward)
        
        # ÏµúÏ¢Ö ÏàòÏùµ Í≥ÑÏÇ∞
        final_balance = info.get('balance', 0)
        initial_balance = info.get('initial_balance', 10_000_000)
        profit = (final_balance - initial_balance) / initial_balance * 100
        
        total_profits.append(profit)
        if profit > 0:
            win_counts += 1
    
    # ÌèâÍ∞Ä ÏßÄÌëú Í≥ÑÏÇ∞
    metrics = {
        'avg_reward': float(np.mean(total_rewards)),
        'std_reward': float(np.std(total_rewards)),
        'avg_profit': float(np.mean(total_profits)),
        'std_profit': float(np.std(total_profits)),
        'win_rate': win_counts / n_episodes,
        'max_profit': float(np.max(total_profits)),
        'min_profit': float(np.min(total_profits))
    }
    
    logger.info("=== RL Agent Evaluation ===")
    logger.info(f"Episodes: {n_episodes}")
    logger.info(f"Avg Reward: {metrics['avg_reward']:.2f} ¬± {metrics['std_reward']:.2f}")
    logger.info(f"Avg Profit: {metrics['avg_profit']:.2f}% ¬± {metrics['std_profit']:.2f}%")
    logger.info(f"Win Rate: {metrics['win_rate']:.1%}")
    logger.info(f"Profit Range: [{metrics['min_profit']:.2f}%, {metrics['max_profit']:.2f}%]")
    
    return metrics
